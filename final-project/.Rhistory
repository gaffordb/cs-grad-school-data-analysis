filter(!is.na(canonical) & !is.na(institution))  %>%
group_by(umbrella, institution) %>%
summarise(mean_authors = mean(numauthors))
#authorship = pubs_reduced %>%
#  filter(!is.na(canonical) & !is.na(institution))  %>%
#  group_by(canonical, institution) %>%
#  summarise(mean_authors = mean(numauthors))
authorship$umbrella = sub("^", "mean_authorcount_", authorship$umbrella )
authorship = authorship %>%
spread(umbrella, mean_authors)
# column_to_rownames(var = "institution")
# Make NA's into 0's
pubs_by_college[is.na(pubs_by_college)] = 0
pubs_by_college = as.data.frame(pubs_by_college)
authorship[is.na(authorship)] = 0
pubs_by_college = left_join(x = pubs_by_college, y = authorship, by = "institution")
pubs_by_college = left_join(pubs_by_college, author_counts)
#std_pubs = scale(pubs_by_college)
nsf_reduced = nsf_awards %>%
group_by(proposed_institution) %>%
summarise(nsf_fellows = n())
nsf_reduced$nsf_fellows[is.na(nsf_reduced$nsf_fellows)] = 0
pubs_by_college = stringdist_left_join(pubs_by_college, nsf_reduced,
by = c("institution" = "proposed_institution"), max_dist = 1)
rankings_data = rankings_data %>% select(c("University.Name", "CS.Score", "CS.rank", "University.Rank", "total_citations")) %>% mutate(University.Name = str_squish(University.Name))
pubs_by_college = stringdist_left_join(pubs_by_college, rankings_data,
by = c("institution" = "University.Name"), max_dist = 1)
pubs_by_college = select(pubs_by_college, -proposed_institution, -University.Name)
#pubs_by_college$nsf_fellows[is.na(pubs_by_college$nsf_fellows)] = 0
pubs_by_college = pubs_by_college %>% column_to_rownames(var = "institution")
nsf_reduced$proposed_institution %in% pubs_by_college$institution
nsf_reduced$proposed_institution[nsf_reduced$proposed_institution %in% pubs_by_college$institution]
nsf_reduced$proposed_institution[!nsf_reduced$proposed_institution %in% pubs_by_college$institution]
#std_pubs = select(pubs_by_college, -CS.Score, -CS.rank, -University.Rank)
pubs_by_college = knnImputation(pubs_by_college, k = 2)
std_pubs = scale(pubs_by_college)
# broken
#combined = semi_join(pubs_by_college, nsf_reduced,
#                     by = c("institution" = "proposed_institution"))
# Here, we scrub some of the cruff out so we can find more interesting trends between the more 'relevant' institutions
pubs_reduced = pubs %>%
filter(!is.na(canonical) & !is.na(institution))  %>%
distinct(title, .keep_all = TRUE) %>%
group_by(institution) %>%
mutate(count = n()) %>%
filter(count > 1214)
#filter(count > 700)
author_counts = pubs_reduced %>% filter(!is.na(canonical) & !is.na(institution))  %>%
distinct(name, .keep_all = TRUE) %>%
group_by(institution) %>%
summarise(author_count = n())
# Before filtering:
length(unique(pubs$institution))
# After filtering:
length(unique(pubs_reduced$institution))
#ggplot(pubs_reduced) + geom_bar(mapping = aes(x = institution)) + xlim(names(sort(table(pubs_reduced$institution), decreasing=TRUE)[1:10])) +
#  theme(axis.text.x  = element_text(angle=45, hjust = 1))
#ggplot(author_counts) + geom_bar(mapping = aes(x = institution), position=position_dodge()) + xlim(names(sort(table(author_counts$institution), decreasing=TRUE)[1:10])) +
#  theme(axis.text.x  = element_text(angle=45, hjust = 1))
pubs_by_college = pubs_reduced %>%
filter(!is.na(canonical) & !is.na(institution))  %>%
group_by(canonical, institution) %>%
count(canonical, institution) %>%
spread(canonical, n)
authorship = pubs_reduced %>%
filter(!is.na(canonical) & !is.na(institution))  %>%
group_by(umbrella, institution) %>%
summarise(mean_authors = mean(numauthors))
#authorship = pubs_reduced %>%
#  filter(!is.na(canonical) & !is.na(institution))  %>%
#  group_by(canonical, institution) %>%
#  summarise(mean_authors = mean(numauthors))
authorship$umbrella = sub("^", "mean_authorcount_", authorship$umbrella )
authorship = authorship %>%
spread(umbrella, mean_authors)
# column_to_rownames(var = "institution")
# Make NA's into 0's
pubs_by_college[is.na(pubs_by_college)] = 0
pubs_by_college = as.data.frame(pubs_by_college)
authorship[is.na(authorship)] = 0
pubs_by_college = left_join(x = pubs_by_college, y = authorship, by = "institution")
pubs_by_college = left_join(pubs_by_college, author_counts)
#std_pubs = scale(pubs_by_college)
nsf_reduced = nsf_awards %>%
group_by(proposed_institution) %>%
summarise(nsf_fellows = n())
nsf_reduced$nsf_fellows[is.na(nsf_reduced$nsf_fellows)] = 0
pubs_by_college = stringdist_left_join(pubs_by_college, nsf_reduced,
by = c("institution" = "proposed_institution"), max_dist = 1)
rankings_data = rankings_data %>% select(c("University.Name", "CS.Score", "CS.rank", "University.Rank", "total_citations")) %>% mutate(University.Name = str_squish(University.Name))
pubs_by_college = stringdist_left_join(pubs_by_college, rankings_data,
by = c("institution" = "University.Name"), max_dist = 1)
pubs_by_college = select(pubs_by_college, -proposed_institution, -University.Name)
#pubs_by_college$nsf_fellows[is.na(pubs_by_college$nsf_fellows)] = 0
pubs_by_college = pubs_by_college %>% column_to_rownames(var = "institution")
#nsf_reduced$proposed_institution %in% pubs_by_college$institution
#nsf_reduced$proposed_institution[nsf_reduced$proposed_institution %in% pubs_by_college$institution]
#nsf_reduced$proposed_institution[!nsf_reduced$proposed_institution %in% pubs_by_college$institution]
#std_pubs = select(pubs_by_college, -CS.Score, -CS.rank, -University.Rank)
pubs_by_college = knnImputation(pubs_by_college, k = 2)
std_pubs = scale(pubs_by_college)
# broken
#combined = semi_join(pubs_by_college, nsf_reduced,
#                     by = c("institution" = "proposed_institution"))
P <- prcomp(std_pubs, scale = TRUE)
colnames(std_pubs)
#fviz_pca_ind(P, repel = FALSE, # Avoid text overlapping (doesn't scale to large datasets)
#                col.ind = std_pubs$CS.Score)  # Individuals color```
fviz_contrib(P, choice = "var", axes = 1)
fviz_contrib(P, choice = "var", axes = 2)
fviz_pca_biplot(P)
fviz_eig(P, addlabels = TRUE)
pam_pubs <- pam(std_pubs, k = 2)
fviz_cluster(pam_pubs) ## Plot the clusters
fviz_nbclust(std_pubs, kmeans, method = "silhouette", k.max = 8)
# maybe also include more vars like num coauthors, num_nsfgrfp people, num_citations
# *** Which conferences indicate overall success? ***
P <- prcomp(std_pubs, scale = TRUE)
colnames(std_pubs)
#fviz_pca_ind(P, repel = FALSE, # Avoid text overlapping (doesn't scale to large datasets)
#                col.ind = std_pubs$CS.Score)  # Individuals color```
fviz_contrib(P, choice = "var", axes = 1)
fviz_contrib(P, choice = "var", axes = 2)
fviz_pca_biplot(P)
fviz_eig(P, addlabels = TRUE)
pam_pubs <- pam(std_pubs, k = 3)
fviz_cluster(pam_pubs) ## Plot the clusters
fviz_nbclust(std_pubs, kmeans, method = "silhouette", k.max = 8)
# maybe also include more vars like num coauthors, num_nsfgrfp people, num_citations
# *** Which conferences indicate overall success? ***
P <- prcomp(std_pubs, scale = TRUE)
colnames(std_pubs)
#fviz_pca_ind(P, repel = FALSE, # Avoid text overlapping (doesn't scale to large datasets)
#                col.ind = std_pubs$CS.Score)  # Individuals color```
fviz_contrib(P, choice = "var", axes = 1)
fviz_contrib(P, choice = "var", axes = 2)
fviz_pca_biplot(P)
fviz_eig(P, addlabels = TRUE)
pam_pubs <- pam(std_pubs, k = 3)
#fviz_cluster(pam_pubs) ## Plot the clusters
fviz_nbclust(std_pubs, kmeans, method = "silhouette", k.max = 8)
# maybe also include more vars like num coauthors, num_nsfgrfp people, num_citations
# *** Which conferences indicate overall success? ***
P <- prcomp(std_pubs, scale = TRUE)
colnames(std_pubs)
#fviz_pca_ind(P, repel = FALSE, # Avoid text overlapping (doesn't scale to large datasets)
#                col.ind = std_pubs$CS.Score)  # Individuals color```
fviz_contrib(P, choice = "var", axes = 1)
fviz_contrib(P, choice = "var", axes = 2)
fviz_pca_biplot(P)
fviz_eig(P, addlabels = TRUE)
pam_pubs <- pam(std_pubs, k = 3)
fviz_cluster(pam_pubs) ## Plot the clusters
#fviz_nbclust(std_pubs, kmeans, method = "silhouette", k.max = 8)
# maybe also include more vars like num coauthors, num_nsfgrfp people, num_citations
# *** Which conferences indicate overall success? ***
# Here, we scrub some of the cruff out so we can find more interesting trends between the more 'relevant' institutions
pubs_reduced = pubs %>%
filter(!is.na(canonical) & !is.na(institution))  %>%
distinct(title, .keep_all = TRUE) %>%
group_by(institution) %>%
mutate(count = n()) %>%
filter(count > 1214)
#filter(count > 700)
author_counts = pubs_reduced %>% filter(!is.na(canonical) & !is.na(institution))  %>%
distinct(name, .keep_all = TRUE) %>%
group_by(institution) %>%
summarise(author_count = n())
# Before filtering:
length(unique(pubs$institution))
# After filtering:
length(unique(pubs_reduced$institution))
#ggplot(pubs_reduced) + geom_bar(mapping = aes(x = institution)) + xlim(names(sort(table(pubs_reduced$institution), decreasing=TRUE)[1:10])) +
#  theme(axis.text.x  = element_text(angle=45, hjust = 1))
#ggplot(author_counts) + geom_bar(mapping = aes(x = institution), position=position_dodge()) + xlim(names(sort(table(author_counts$institution), decreasing=TRUE)[1:10])) +
#  theme(axis.text.x  = element_text(angle=45, hjust = 1))
pubs_by_college = pubs_reduced %>%
filter(!is.na(canonical) & !is.na(institution))  %>%
group_by(area, institution) %>%
count(area, institution) %>%
spread(area, n)
authorship = pubs_reduced %>%
filter(!is.na(canonical) & !is.na(institution))  %>%
group_by(umbrella, institution) %>%
summarise(mean_authors = mean(numauthors))
#authorship = pubs_reduced %>%
#  filter(!is.na(canonical) & !is.na(institution))  %>%
#  group_by(canonical, institution) %>%
#  summarise(mean_authors = mean(numauthors))
authorship$umbrella = sub("^", "mean_authorcount_", authorship$umbrella )
authorship = authorship %>%
spread(umbrella, mean_authors)
# column_to_rownames(var = "institution")
# Make NA's into 0's
pubs_by_college[is.na(pubs_by_college)] = 0
pubs_by_college = as.data.frame(pubs_by_college)
authorship[is.na(authorship)] = 0
pubs_by_college = left_join(x = pubs_by_college, y = authorship, by = "institution")
pubs_by_college = left_join(pubs_by_college, author_counts)
#std_pubs = scale(pubs_by_college)
nsf_reduced = nsf_awards %>%
group_by(proposed_institution) %>%
summarise(nsf_fellows = n())
nsf_reduced$nsf_fellows[is.na(nsf_reduced$nsf_fellows)] = 0
pubs_by_college = stringdist_left_join(pubs_by_college, nsf_reduced,
by = c("institution" = "proposed_institution"), max_dist = 1)
rankings_data = rankings_data %>% select(c("University.Name", "CS.Score", "CS.rank", "University.Rank", "total_citations")) %>% mutate(University.Name = str_squish(University.Name))
pubs_by_college = stringdist_left_join(pubs_by_college, rankings_data,
by = c("institution" = "University.Name"), max_dist = 1)
pubs_by_college = select(pubs_by_college, -proposed_institution, -University.Name)
#pubs_by_college$nsf_fellows[is.na(pubs_by_college$nsf_fellows)] = 0
pubs_by_college = pubs_by_college %>% column_to_rownames(var = "institution")
#nsf_reduced$proposed_institution %in% pubs_by_college$institution
#nsf_reduced$proposed_institution[nsf_reduced$proposed_institution %in% pubs_by_college$institution]
#nsf_reduced$proposed_institution[!nsf_reduced$proposed_institution %in% pubs_by_college$institution]
#std_pubs = select(pubs_by_college, -CS.Score, -CS.rank, -University.Rank)
pubs_by_college = knnImputation(pubs_by_college, k = 2)
std_pubs = scale(pubs_by_college)
# broken
#combined = semi_join(pubs_by_college, nsf_reduced,
#                     by = c("institution" = "proposed_institution"))
P <- prcomp(std_pubs, scale = TRUE)
colnames(std_pubs)
#fviz_pca_ind(P, repel = FALSE, # Avoid text overlapping (doesn't scale to large datasets)
#                col.ind = std_pubs$CS.Score)  # Individuals color```
fviz_contrib(P, choice = "var", axes = 1)
fviz_contrib(P, choice = "var", axes = 2)
fviz_pca_biplot(P)
fviz_eig(P, addlabels = TRUE)
pam_pubs <- pam(std_pubs, k = 3)
fviz_cluster(pam_pubs) ## Plot the clusters
#fviz_nbclust(std_pubs, kmeans, method = "silhouette", k.max = 8)
# maybe also include more vars like num coauthors, num_nsfgrfp people, num_citations
# *** Which conferences indicate overall success? ***
fviz_nbclust(std_pubs, kmeans, method = "silhouette", k.max = 8)
P <- prcomp(std_pubs, scale = TRUE)
colnames(std_pubs)
#fviz_pca_ind(P, repel = FALSE, # Avoid text overlapping (doesn't scale to large datasets)
#                col.ind = std_pubs$CS.Score)  # Individuals color```
fviz_contrib(P, choice = "var", axes = 1)
fviz_contrib(P, choice = "var", axes = 2)
fviz_pca_biplot(P)
fviz_eig(P, addlabels = TRUE)
pam_pubs <- pam(std_pubs, k = 3)
fviz_cluster(pam_pubs) ## Plot the clusters
fviz_nbclust(std_pubs, kmeans, method = "silhouette", k.max = 8)
# maybe also include more vars like num coauthors, num_nsfgrfp people, num_citations
# *** Which conferences indicate overall success? ***
coef(fit_lasso$finalModel, s = exp(fit_lasso$finalModel$lambdaOpt))
pairs(pubs_by_college)
pairs(pubs_by_college)
fit.control <- trainControl(method = "repeatedcv", number = 5, repeats = 10)
fit_lasso <- train(CS.Score ~ . -CS.rank, data = pubs_by_college, method = "glmnet", trControl = fit.control, trace = FALSE)
coef(fit_lasso$finalModel, s = exp(fit_lasso$finalModel$lambdaOpt))
fit_lasso
# Here, we scrub some of the cruff out so we can find more interesting trends between the more 'relevant' institutions
pubs_reduced = pubs %>%
filter(!is.na(canonical) & !is.na(institution))  %>%
distinct(title, .keep_all = TRUE) %>%
group_by(institution) %>%
mutate(count = n()) %>%
filter(count > 1214)
#filter(count > 700)
author_counts = pubs_reduced %>% filter(!is.na(canonical) & !is.na(institution))  %>%
distinct(name, .keep_all = TRUE) %>%
group_by(institution) %>%
summarise(author_count = n())
# Before filtering:
length(unique(pubs$institution))
# After filtering:
length(unique(pubs_reduced$institution))
#ggplot(pubs_reduced) + geom_bar(mapping = aes(x = institution)) + xlim(names(sort(table(pubs_reduced$institution), decreasing=TRUE)[1:10])) +
#  theme(axis.text.x  = element_text(angle=45, hjust = 1))
#ggplot(author_counts) + geom_bar(mapping = aes(x = institution), position=position_dodge()) + xlim(names(sort(table(author_counts$institution), decreasing=TRUE)[1:10])) +
#  theme(axis.text.x  = element_text(angle=45, hjust = 1))
pubs_by_college = pubs_reduced %>%
filter(!is.na(canonical) & !is.na(institution))  %>%
group_by(umbrella, institution) %>%
count(umbrella, institution) %>%
spread(umbrella, n)
authorship = pubs_reduced %>%
filter(!is.na(canonical) & !is.na(institution))  %>%
group_by(umbrella, institution) %>%
summarise(mean_authors = mean(numauthors))
#authorship = pubs_reduced %>%
#  filter(!is.na(canonical) & !is.na(institution))  %>%
#  group_by(canonical, institution) %>%
#  summarise(mean_authors = mean(numauthors))
authorship$umbrella = sub("^", "mean_authorcount_", authorship$umbrella )
authorship = authorship %>%
spread(umbrella, mean_authors)
# column_to_rownames(var = "institution")
# Make NA's into 0's
pubs_by_college[is.na(pubs_by_college)] = 0
pubs_by_college = as.data.frame(pubs_by_college)
authorship[is.na(authorship)] = 0
pubs_by_college = left_join(x = pubs_by_college, y = authorship, by = "institution")
pubs_by_college = left_join(pubs_by_college, author_counts)
#std_pubs = scale(pubs_by_college)
nsf_reduced = nsf_awards %>%
group_by(proposed_institution) %>%
summarise(nsf_fellows = n())
nsf_reduced$nsf_fellows[is.na(nsf_reduced$nsf_fellows)] = 0
pubs_by_college = stringdist_left_join(pubs_by_college, nsf_reduced,
by = c("institution" = "proposed_institution"), max_dist = 1)
rankings_data = rankings_data %>% select(c("University.Name", "CS.Score", "CS.rank", "University.Rank", "total_citations")) %>% mutate(University.Name = str_squish(University.Name))
pubs_by_college = stringdist_left_join(pubs_by_college, rankings_data,
by = c("institution" = "University.Name"), max_dist = 1)
pubs_by_college = select(pubs_by_college, -proposed_institution, -University.Name)
#pubs_by_college$nsf_fellows[is.na(pubs_by_college$nsf_fellows)] = 0
pubs_by_college = pubs_by_college %>% column_to_rownames(var = "institution")
#nsf_reduced$proposed_institution %in% pubs_by_college$institution
#nsf_reduced$proposed_institution[nsf_reduced$proposed_institution %in% pubs_by_college$institution]
#nsf_reduced$proposed_institution[!nsf_reduced$proposed_institution %in% pubs_by_college$institution]
#std_pubs = select(pubs_by_college, -CS.Score, -CS.rank, -University.Rank)
pubs_by_college = knnImputation(pubs_by_college, k = 2)
std_pubs = scale(pubs_by_college)
# broken
#combined = semi_join(pubs_by_college, nsf_reduced,
#                     by = c("institution" = "proposed_institution"))
P <- prcomp(std_pubs, scale = TRUE)
colnames(std_pubs)
#fviz_pca_ind(P, repel = FALSE, # Avoid text overlapping (doesn't scale to large datasets)
#                col.ind = std_pubs$CS.Score)  # Individuals color```
fviz_contrib(P, choice = "var", axes = 1)
fviz_contrib(P, choice = "var", axes = 2)
fviz_pca_biplot(P)
fviz_eig(P, addlabels = TRUE)
pam_pubs <- pam(std_pubs, k = 3)
fviz_cluster(pam_pubs) ## Plot the clusters
fviz_nbclust(std_pubs, kmeans, method = "silhouette", k.max = 8)
# maybe also include more vars like num coauthors, num_nsfgrfp people, num_citations
# *** Which conferences indicate overall success? ***
# Here, we scrub some of the cruff out so we can find more interesting trends between the more 'relevant' institutions
pubs_reduced = pubs %>%
filter(!is.na(canonical) & !is.na(institution))  %>%
distinct(title, .keep_all = TRUE) %>%
group_by(institution) %>%
mutate(count = n()) %>%
filter(count > 1214)
#filter(count > 700)
author_counts = pubs_reduced %>% filter(!is.na(canonical) & !is.na(institution))  %>%
distinct(name, .keep_all = TRUE) %>%
group_by(institution) %>%
summarise(author_count = n())
# Before filtering:
length(unique(pubs$institution))
# After filtering:
length(unique(pubs_reduced$institution))
#ggplot(pubs_reduced) + geom_bar(mapping = aes(x = institution)) + xlim(names(sort(table(pubs_reduced$institution), decreasing=TRUE)[1:10])) +
#  theme(axis.text.x  = element_text(angle=45, hjust = 1))
#ggplot(author_counts) + geom_bar(mapping = aes(x = institution), position=position_dodge()) + xlim(names(sort(table(author_counts$institution), decreasing=TRUE)[1:10])) +
#  theme(axis.text.x  = element_text(angle=45, hjust = 1))
pubs_by_college = pubs_reduced %>%
filter(!is.na(canonical) & !is.na(institution))  %>%
group_by(umbrella, institution) %>%
count(umbrella, institution) %>%
spread(umbrella, n)
authorship = pubs_reduced %>%
filter(!is.na(canonical) & !is.na(institution))  %>%
group_by(umbrella, institution) %>%
summarise(mean_authors = mean(numauthors))
#authorship = pubs_reduced %>%
#  filter(!is.na(canonical) & !is.na(institution))  %>%
#  group_by(canonical, institution) %>%
#  summarise(mean_authors = mean(numauthors))
authorship$umbrella = sub("^", "mean_authorcount_", authorship$umbrella )
authorship = authorship %>%
spread(umbrella, mean_authors)
# column_to_rownames(var = "institution")
# Make NA's into 0's
pubs_by_college[is.na(pubs_by_college)] = 0
pubs_by_college = as.data.frame(pubs_by_college)
authorship[is.na(authorship)] = 0
#pubs_by_college = left_join(x = pubs_by_college, y = authorship, by = "institution")
pubs_by_college = left_join(pubs_by_college, author_counts)
#std_pubs = scale(pubs_by_college)
nsf_reduced = nsf_awards %>%
group_by(proposed_institution) %>%
summarise(nsf_fellows = n())
nsf_reduced$nsf_fellows[is.na(nsf_reduced$nsf_fellows)] = 0
pubs_by_college = stringdist_left_join(pubs_by_college, nsf_reduced,
by = c("institution" = "proposed_institution"), max_dist = 1)
rankings_data = rankings_data %>% select(c("University.Name", "CS.Score", "CS.rank", "University.Rank", "total_citations")) %>% mutate(University.Name = str_squish(University.Name))
pubs_by_college = stringdist_left_join(pubs_by_college, rankings_data,
by = c("institution" = "University.Name"), max_dist = 1)
pubs_by_college = select(pubs_by_college, -proposed_institution, -University.Name)
#pubs_by_college$nsf_fellows[is.na(pubs_by_college$nsf_fellows)] = 0
pubs_by_college = pubs_by_college %>% column_to_rownames(var = "institution")
#nsf_reduced$proposed_institution %in% pubs_by_college$institution
#nsf_reduced$proposed_institution[nsf_reduced$proposed_institution %in% pubs_by_college$institution]
#nsf_reduced$proposed_institution[!nsf_reduced$proposed_institution %in% pubs_by_college$institution]
#std_pubs = select(pubs_by_college, -CS.Score, -CS.rank, -University.Rank)
pubs_by_college = knnImputation(pubs_by_college, k = 2)
std_pubs = scale(pubs_by_college)
# broken
#combined = semi_join(pubs_by_college, nsf_reduced,
#                     by = c("institution" = "proposed_institution"))
P <- prcomp(std_pubs, scale = TRUE)
colnames(std_pubs)
#fviz_pca_ind(P, repel = FALSE, # Avoid text overlapping (doesn't scale to large datasets)
#                col.ind = std_pubs$CS.Score)  # Individuals color```
fviz_contrib(P, choice = "var", axes = 1)
fviz_contrib(P, choice = "var", axes = 2)
fviz_pca_biplot(P)
fviz_eig(P, addlabels = TRUE)
pam_pubs <- pam(std_pubs, k = 3)
fviz_cluster(pam_pubs) ## Plot the clusters
fviz_nbclust(std_pubs, kmeans, method = "silhouette", k.max = 8)
# maybe also include more vars like num coauthors, num_nsfgrfp people, num_citations
# *** Which conferences indicate overall success? ***
fit.control <- trainControl(method = "repeatedcv", number = 5, repeats = 10)
fit_lasso <- train(CS.Score ~ . -CS.rank, data = pubs_by_college, method = "glmnet", trControl = fit.control, trace = FALSE)
coef(fit_lasso$finalModel, s = exp(fit_lasso$finalModel$lambdaOpt))
fit_lasso
# Here, we scrub some of the cruff out so we can find more interesting trends between the more 'relevant' institutions
pubs_reduced = pubs %>%
filter(!is.na(canonical) & !is.na(institution))  %>%
distinct(title, .keep_all = TRUE) %>%
group_by(institution) %>%
mutate(count = n()) %>%
#filter(count > 1214)
filter(count > 700)
author_counts = pubs_reduced %>% filter(!is.na(canonical) & !is.na(institution))  %>%
distinct(name, .keep_all = TRUE) %>%
group_by(institution) %>%
summarise(author_count = n())
# Before filtering:
length(unique(pubs$institution))
# After filtering:
length(unique(pubs_reduced$institution))
ggplot(pubs_reduced) + geom_bar(mapping = aes(x = institution)) + xlim(names(sort(table(pubs_reduced$institution), decreasing=TRUE)[1:10])) +
theme(axis.text.x  = element_text(angle=45, hjust = 1))
#ggplot(author_counts) + geom_bar(mapping = aes(x = institution), position=position_dodge()) + xlim(names(sort(table(author_counts$institution), decreasing=TRUE)[1:10])) +
#  theme(axis.text.x  = element_text(angle=45, hjust = 1))
pubs_by_college = pubs_reduced %>%
filter(!is.na(canonical) & !is.na(institution))  %>%
group_by(umbrella, institution) %>%
count(umbrella, institution) %>%
spread(umbrella, n)
authorship = pubs_reduced %>%
filter(!is.na(canonical) & !is.na(institution))  %>%
group_by(umbrella, institution) %>%
summarise(mean_authors = mean(numauthors))
#authorship = pubs_reduced %>%
#  filter(!is.na(canonical) & !is.na(institution))  %>%
#  group_by(canonical, institution) %>%
#  summarise(mean_authors = mean(numauthors))
authorship$umbrella = sub("^", "mean_authorcount_", authorship$umbrella )
authorship = authorship %>%
spread(umbrella, mean_authors)
# column_to_rownames(var = "institution")
# Make NA's into 0's
pubs_by_college[is.na(pubs_by_college)] = 0
pubs_by_college = as.data.frame(pubs_by_college)
authorship[is.na(authorship)] = 0
pubs_by_college = left_join(x = pubs_by_college, y = authorship, by = "institution")
pubs_by_college = left_join(pubs_by_college, author_counts)
#std_pubs = scale(pubs_by_college)
nsf_reduced = nsf_awards %>%
group_by(proposed_institution) %>%
summarise(nsf_fellows = n())
nsf_reduced$nsf_fellows[is.na(nsf_reduced$nsf_fellows)] = 0
pubs_by_college = stringdist_left_join(pubs_by_college, nsf_reduced,
by = c("institution" = "proposed_institution"), max_dist = 1)
rankings_data = rankings_data %>% select(c("University.Name", "CS.Score", "CS.rank", "University.Rank", "total_citations")) %>% mutate(University.Name = str_squish(University.Name))
pubs_by_college = stringdist_left_join(pubs_by_college, rankings_data,
by = c("institution" = "University.Name"), max_dist = 1)
pubs_by_college = select(pubs_by_college, -proposed_institution, -University.Name)
#pubs_by_college$nsf_fellows[is.na(pubs_by_college$nsf_fellows)] = 0
pubs_by_college = pubs_by_college %>% column_to_rownames(var = "institution")
#nsf_reduced$proposed_institution %in% pubs_by_college$institution
#nsf_reduced$proposed_institution[nsf_reduced$proposed_institution %in% pubs_by_college$institution]
#nsf_reduced$proposed_institution[!nsf_reduced$proposed_institution %in% pubs_by_college$institution]
#std_pubs = select(pubs_by_college, -CS.Score, -CS.rank, -University.Rank)
pubs_by_college = knnImputation(pubs_by_college, k = 2)
std_pubs = scale(pubs_by_college)
# broken
#combined = semi_join(pubs_by_college, nsf_reduced,
#                     by = c("institution" = "proposed_institution"))
P <- prcomp(std_pubs, scale = TRUE)
colnames(std_pubs)
#fviz_pca_ind(P, repel = FALSE, # Avoid text overlapping (doesn't scale to large datasets)
#                col.ind = std_pubs$CS.Score)  # Individuals color```
fviz_contrib(P, choice = "var", axes = 1)
fviz_contrib(P, choice = "var", axes = 2)
fviz_pca_biplot(P)
fviz_eig(P, addlabels = TRUE)
pam_pubs <- pam(std_pubs, k = 2)
fviz_cluster(pam_pubs) ## Plot the clusters
fviz_nbclust(std_pubs, kmeans, method = "silhouette", k.max = 8)
# maybe also include more vars like num coauthors, num_nsfgrfp people, num_citations
# *** Which conferences indicate overall success? ***
fit.control <- trainControl(method = "repeatedcv", number = 5, repeats = 10)
fit_lasso <- train(CS.Score ~ . -CS.rank -, data = pubs_by_college, method = "glmnet", trControl = fit.control, trace = FALSE)
fit.control <- trainControl(method = "repeatedcv", number = 5, repeats = 10)
fit_lasso <- train(CS.Score ~ . -CS.rank, data = pubs_by_college, method = "glmnet", trControl = fit.control, trace = FALSE)
coef(fit_lasso$finalModel, s = exp(fit_lasso$finalModel$lambdaOpt))
fit_lasso
P <- prcomp(std_pubs, scale = TRUE)
#colnames(std_pubs)
#fviz_pca_ind(P, repel = FALSE, # Avoid text overlapping (doesn't scale to large datasets)
#                col.ind = std_pubs$CS.Score)  # Individuals color```
fviz_contrib(P, choice = "var", axes = 1)
fviz_contrib(P, choice = "var", axes = 2)
fviz_pca_biplot(P)
fviz_eig(P, addlabels = TRUE)
pam_pubs <- pam(std_pubs, k = 3)
fviz_cluster(pam_pubs) ## Plot the clusters
#fviz_nbclust(std_pubs, kmeans, method = "silhouette", k.max = 8)
# maybe also include more vars like num coauthors, num_nsfgrfp people, num_citations
# *** Which conferences indicate overall success? ***
